{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict\n",
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f151fb95bc2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd368d9696a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f093cff969ddb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'input_size': 6,  # Number of input features\n",
    "    'sequence_length': 10,  # Number of time steps to look back\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 100,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1241d98008b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindSpeedDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, sequence_length: int):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.data[idx:idx + self.sequence_length, :-1]\n",
    "        y = self.data[idx + self.sequence_length - 1, -1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4d9b41241efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame) -> Tuple[np.ndarray, MinMaxScaler]:\n",
    "    \"\"\"\n",
    "    Preprocess the data with both scaling and statistical normalization\n",
    "    \"\"\"\n",
    "    # First apply MinMax scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    # Apply statistical normalization\n",
    "    mean = np.mean(scaled_data, axis=0)\n",
    "    std = np.std(scaled_data, axis=0)\n",
    "    normalized_data = (scaled_data - mean) / (std + 1e-8)\n",
    "\n",
    "    return normalized_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe15e13f95ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data = pd.read_csv('data.csv')\n",
    "display(data.head())\n",
    "print(\"\\nData shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dcbdcc395d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input features distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "data.boxplot()\n",
    "plt.title('Distribution of Wind Speeds at Different Heights')\n",
    "plt.ylabel('Wind Speed (m/s)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3fa8eb6a44350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Add batch normalization for input\n",
    "        self.input_bn = nn.BatchNorm1d(input_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Add batch normalization after LSTM\n",
    "        self.hidden_bn = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Apply input batch normalization\n",
    "        x = x.view(-1, x.size(-1))\n",
    "        x = self.input_bn(x)\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Apply batch normalization on the output features\n",
    "        out = out[:, -1, :]\n",
    "        out = self.hidden_bn(out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682bdf6880a7f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparrowSearch:\n",
    "    def __init__(self, n_particles: int, max_iter: int, param_bounds: Dict):\n",
    "        self.n_particles = n_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.param_bounds = param_bounds\n",
    "        self.best_solution = None\n",
    "        self.best_fitness = float('inf')\n",
    "\n",
    "    def initialize_population(self) -> List[Dict]:\n",
    "        population = []\n",
    "        for _ in range(self.n_particles):\n",
    "            particle = {}\n",
    "            for param, (low, high) in self.param_bounds.items():\n",
    "                if isinstance(low, int) and isinstance(high, int):\n",
    "                    particle[param] = random.randint(low, high)\n",
    "                else:\n",
    "                    particle[param] = random.uniform(low, high)\n",
    "            population.append(particle)\n",
    "        return population\n",
    "\n",
    "    def update_position(self, particle: Dict, r2: float, alarm_value: float) -> Dict:\n",
    "        new_particle = particle.copy()\n",
    "        for param, (low, high) in self.param_bounds.items():\n",
    "            if random.random() < alarm_value:\n",
    "                if isinstance(low, int) and isinstance(high, int):\n",
    "                    new_particle[param] = random.randint(low, high)\n",
    "                else:\n",
    "                    new_particle[param] = random.uniform(low, high)\n",
    "            else:\n",
    "                step = r2 * (self.best_solution[param] - particle[param])\n",
    "                new_value = particle[param] + step\n",
    "                if isinstance(low, int) and isinstance(high, int):\n",
    "                    new_value = int(round(new_value))\n",
    "                new_particle[param] = max(low, min(high, new_value))\n",
    "        return new_particle\n",
    "\n",
    "    def optimize(self, fitness_func) -> Tuple[Dict, float]:\n",
    "        population = self.initialize_population()\n",
    "        history = []\n",
    "\n",
    "        # Add tqdm progress bar\n",
    "        pbar = tqdm(range(self.max_iter), desc='Sparrow Search Progress')\n",
    "        \n",
    "        for iteration in pbar:\n",
    "            alarm_value = 0.5 - (0.5 * iteration / self.max_iter)\n",
    "\n",
    "            for i, particle in enumerate(population):\n",
    "                fitness = fitness_func(particle)\n",
    "\n",
    "                if fitness < self.best_fitness:\n",
    "                    self.best_fitness = fitness\n",
    "                    self.best_solution = particle.copy()\n",
    "                    # Update progress bar description with current best fitness\n",
    "                    pbar.set_postfix({'Best Fitness': f'{self.best_fitness:.6f}'})\n",
    "\n",
    "            r2 = random.random()\n",
    "            population = [self.update_position(p, r2, alarm_value) for p in population]\n",
    "\n",
    "            history.append(self.best_fitness)\n",
    "            logger.info(f\"Iteration {iteration + 1}/{self.max_iter}, Best fitness: {self.best_fitness:.6f}\")\n",
    "\n",
    "        # Plot optimization history\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history)\n",
    "        plt.title('Sparrow Search Optimization History')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Best Fitness')\n",
    "        plt.show()\n",
    "\n",
    "        return self.best_solution, self.best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944b5c525164853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(\n",
    "    scaled_data: np.ndarray,\n",
    "    sequence_length: int,\n",
    "    batch_size: int,\n",
    "    train_ratio: float = 0.8\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train_size = int(len(scaled_data) * train_ratio)\n",
    "    train_data = scaled_data[:train_size]\n",
    "    val_data = scaled_data[train_size:]\n",
    "\n",
    "    train_dataset = WindSpeedDataset(train_data, sequence_length)\n",
    "    val_dataset = WindSpeedDataset(val_data, sequence_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7550aea8dfca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    epochs: int\n",
    ") -> Dict[str, List[float]]:\n",
    "    model = model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                val_loss = criterion(outputs, batch_y)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            logger.info(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dffbd9e7eea91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "scaled_data, scaler = preprocess_data(data)\n",
    "train_loader, val_loader = create_data_loaders(\n",
    "    scaled_data,\n",
    "    CONFIG['sequence_length'],\n",
    "    CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "# Define parameter bounds for Sparrow Search\n",
    "param_bounds = {\n",
    "    'hidden_size': (32, 256),\n",
    "    'num_layers': (1, 4),\n",
    "    'dropout': (0.0, 0.5),\n",
    "    'learning_rate': (0.0001, 0.01)\n",
    "}\n",
    "\n",
    "# Initialize Sparrow Search\n",
    "sparrow = SparrowSearch(n_particles=20, max_iter=30, param_bounds=param_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855b117748c5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness function\n",
    "def fitness_function(params):\n",
    "    model = LSTM(\n",
    "        input_size=CONFIG['input_size'],\n",
    "        hidden_size=int(params['hidden_size']),\n",
    "        num_layers=int(params['num_layers']),\n",
    "        dropout=params['dropout']\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    history = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        criterion, optimizer,\n",
    "        CONFIG['device'], CONFIG['epochs']\n",
    "    )\n",
    "\n",
    "    return np.mean(history['val_loss'])\n",
    "\n",
    "# Run optimization\n",
    "best_params, best_fitness = sparrow.optimize(fitness_function)\n",
    "print(\"\\nBest parameters found:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12483c7eed1db975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    scaler: MinMaxScaler,\n",
    "    device: str\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))[:, 0]\n",
    "    actuals = scaler.inverse_transform(np.array(actuals).reshape(-1, 1))[:, 0]\n",
    "\n",
    "    metrics = {\n",
    "        'mse': mean_squared_error(actuals, predictions),\n",
    "        'mae': mean_absolute_error(actuals, predictions),\n",
    "        'r2': r2_score(actuals, predictions)\n",
    "    }\n",
    "\n",
    "    # Plot predictions vs actuals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(actuals, predictions, alpha=0.5)\n",
    "    plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'r--')\n",
    "    plt.xlabel('Actual Wind Speed')\n",
    "    plt.ylabel('Predicted Wind Speed')\n",
    "    plt.title('Predictions vs Actuals')\n",
    "    plt.show()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9efabd3b97aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate vanilla LSTM\n",
    "vanilla_lstm = LSTM(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "vanilla_optimizer = torch.optim.Adam(vanilla_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "vanilla_history = train_model(\n",
    "    vanilla_lstm, train_loader, val_loader,\n",
    "    criterion, vanilla_optimizer,\n",
    "    CONFIG['device'], CONFIG['epochs']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fcb9942d0bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate optimized LSTM\n",
    "optimized_lstm = LSTM(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=int(best_params['hidden_size']),\n",
    "    num_layers=int(best_params['num_layers']),\n",
    "    dropout=best_params['dropout']\n",
    ")\n",
    "optimized_optimizer = torch.optim.Adam(\n",
    "    optimized_lstm.parameters(),\n",
    "    lr=best_params['learning_rate']\n",
    ")\n",
    "\n",
    "optimized_history = train_model(\n",
    "    optimized_lstm, train_loader, val_loader,\n",
    "    criterion, optimized_optimizer,\n",
    "    CONFIG['device'], CONFIG['epochs']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd28d52e26c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and visualize results\n",
    "print(\"\\nVanilla LSTM Metrics:\")\n",
    "vanilla_metrics = evaluate_model(vanilla_lstm, val_loader, scaler, CONFIG['device'])\n",
    "for metric, value in vanilla_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nOptimized LSTM Metrics:\")\n",
    "optimized_metrics = evaluate_model(optimized_lstm, val_loader, scaler, CONFIG['device'])\n",
    "for metric, value in optimized_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949040960629903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(vanilla_history['val_loss'], label='Vanilla LSTM', alpha=0.8)\n",
    "plt.plot(optimized_history['val_loss'], label='Optimized LSTM', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Training History Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7893d01acbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Save Models and Results\n",
    "\n",
    "# %%\n",
    "# Save models\n",
    "def save_model(model, filename):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': CONFIG,\n",
    "        'scaler': scaler\n",
    "    }, filename)\n",
    "\n",
    "save_model(vanilla_lstm, 'vanilla_lstm.pth')\n",
    "save_model(optimized_lstm, 'optimized_lstm.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc6142edd6fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics comparison\n",
    "results = {\n",
    "    'vanilla_lstm': vanilla_metrics,\n",
    "    'optimized_lstm': optimized_metrics,\n",
    "    'best_parameters': best_params\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "results_df = pd.DataFrame({\n",
    "    'Vanilla LSTM': vanilla_metrics,\n",
    "    'Optimized LSTM': optimized_metrics\n",
    "})\n",
    "\n",
    "display(HTML(results_df.to_html()))\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Prediction Function for New Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9425349b55e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def predict_wind_speed(model, input_data, scaler, device=CONFIG['device']):\n",
    "    \"\"\"\n",
    "    Make predictions for new input data\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        input_data: Input features (should match the expected input shape)\n",
    "        scaler: Fitted scaler object\n",
    "        device: Computing device (cuda/cpu)\n",
    "\n",
    "    Returns:\n",
    "        Predicted wind speed value\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Preprocess input\n",
    "        scaled_input = scaler.transform(input_data)\n",
    "        input_tensor = torch.FloatTensor(scaled_input).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get prediction\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        # Inverse transform prediction\n",
    "        prediction = scaler.inverse_transform(\n",
    "            output.cpu().numpy().reshape(-1, 1)\n",
    "        )[0, 0]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# For new data point:\n",
    "new_data = np.array([[...]])  # Array with wind speeds at different heights\n",
    "predicted_speed = predict_wind_speed(optimized_lstm, new_data, scaler)\n",
    "print(f\"Predicted wind speed at 107m: {predicted_speed:.2f} m/s\")\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Conclusion and Model Analysis\n",
    "#\n",
    "# The notebook implements and compares two LSTM models for wind speed prediction:\n",
    "# 1. Vanilla LSTM with standard hyperparameters\n",
    "# 2. Optimized LSTM with Sparrow Search algorithm\n",
    "#\n",
    "# Key features implemented:\n",
    "# - Batch normalization for improved training stability\n",
    "# - Mixed precision training for better performance\n",
    "# - Comprehensive evaluation metrics\n",
    "# - Visualization of results\n",
    "# - Model saving and loading functionality\n",
    "#\n",
    "# The optimized model shows [improvement/degradation] in performance compared to the vanilla model, particularly in terms of [specific metrics]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
